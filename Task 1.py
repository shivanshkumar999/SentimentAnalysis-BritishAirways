# -*- coding: utf-8 -*-
"""web scrapping

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EQdb_108qHdyRobuNCMn3-0eeom7Kt6w

# **Create Dataset**
"""

import requests
from bs4 import BeautifulSoup

import pandas as pd

base_url = "https://www.airlinequality.com/airline-reviews/british-airways"
pages = 11
page_size = 100

data_ = []

for i in range(0, pages+1):
  print(f"Page {i} scraped.")
  url = f"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}"
  r = requests.get(url)
  content = r.content
  parsed_content = BeautifulSoup(content, 'html.parser')
  for para in parsed_content.find_all("div", {"class": "text_content"}):
    data_.append(para.get_text().split('|')[1])

print(f"->->->->->-> WOW!! {len(data_)} reviews collected in total.!!<-<-<-<-<-<-")

data = pd.DataFrame()
data['Reviews'] = data_
data

"""


---


# Data Cleaning Started.

## 1. Convert our data into the Lowercase."""

import spacy
import pandas as pd

nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

data['newReviews'] = data['Reviews'].apply(lambda x:" ".join(x.lower() for x in x.split()))
data['newReviews'].head()

"""## 2. Remove punctuation and perform Tokenization"""

import nltk

nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk import pos_tag

nltk.download('stopwords')
from nltk.corpus import stopwords

nltk.download('wordnet')
from nltk.corpus import wordnet


data['newReviews'] = data['newReviews'].str.replace('[^\w\s]', ' ')
data['newReviews']

"""## 3. Removing Stopwords"""

from wordcloud import WordCloud, STOPWORDS
stop = stopwords.words('english')
data['newReviews'] = data['newReviews'].apply(lambda x:" ".join(x for x in x.split() if x not in stop))
data['newReviews']

"""## 4. Performing Lemmatization"""

from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

def space(comment):
  doc = nlp(comment)
  return " ".join([token.lemma_ for token in doc])

data['newReviews'] = data['newReviews'].apply(space)
data['newReviews']

"""## 5. Performing Sentiment Analysis using VADER"""

# !pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

def vadersentimentanalysis(review):
  vs = analyzer.polarity_scores(review)
  return vs['compound']

data['newReviews_Pol_Scores'] = data['newReviews'].apply(vadersentimentanalysis)

def vaderanalysis(compound):
  if compound >= 0.5:
    return "Positive"
  if compound < 0:
    return "Negative"
  else:
    return "Neutral"

data['Analysis'] = data['newReviews_Pol_Scores'].apply(vaderanalysis)
data['Analysis']

"""## 6. Plotting a PIE chart using Matplotlib"""

import matplotlib.pyplot as plt

vader_counts = data['Analysis'].value_counts()
print(vader_counts)

plt.pie(vader_counts.values, labels = vader_counts.index, explode = [0.01, 0.01, 0.1],  autopct = '%1.1f%%', colors = ['#D2C1CE','#E1CEC9', '#DFD8DC'])
plt.show()

"""## 7. Making a Word Cloud"""

from wordcloud import WordCloud, STOPWORDS
stopwords = set(STOPWORDS)

# remove useless words from the text

from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

# creating word cloud

wc = WordCloud(stopwords = stop_words, collocations = False, max_font_size = 30, max_words = 100, background_color = 'white')
wc.generate(' '.join(data.Reviews))
plt.figure(figsize=(10, 12))
plt.imshow(wc, interpolation = "bilinear")
plt.axis('off')

"""---




"""